import os
import time
import math
import logging
import tiktoken
from collections import deque
from typing import List, Dict, Tuple
from openai import OpenAI, BadRequestError
try:
    from openai import RateLimitError, APIStatusError
except Exception:
    RateLimitError = tuple()
    APIStatusError = tuple()

logger = logging.getLogger(__name__)

MODEL = os.getenv("OPENAI_MODEL", "gpt-4o-mini")  # ÈÄöÂ∏∏ÂøúÁ≠îÁî®
MODEL_SUMMARY = os.getenv("OPENAI_MODEL_SUMMARY", "gpt-4.1-nano")  # Ë¶ÅÁ¥ÑÂ∞ÇÁî®
MODEL_CONTEXT = 128_000
BUDGET_RATIO = 0.80             # ‰∏äÈôê„ÅÆ 80%
OUTPUT_RESERVE = 1024           # ÂøúÁ≠îÁî®„Å´ÊúÄ‰Ωé„Åì„Çå„Å†„Åë„ÅØÁ¢∫‰øù
TARGET_BUDGET = max(16_000, int(MODEL_CONTEXT * BUDGET_RATIO) - OUTPUT_RESERVE)

client = OpenAI()

CORE_PROMPT = os.getenv("OPENAI_CORE_PROMPT", "").strip()
ROLE_PROMPT = os.getenv("OPENAI_ROLE_PROMPT", "").strip()
FALLBACK_GENERIC   = os.getenv("FALLBACK_GENERIC", "Âá¶ÁêÜ‰∏≠„Å´ÂïèÈ°å„ÅåÁô∫Áîü„Åó„Åæ„Åó„Åü„ÄÇÊÅê„ÇåÂÖ•„Çä„Åæ„Åô„Åå„ÄÅ„ÇÇ„ÅÜ‰∏ÄÂ∫¶ÂÖ•Âäõ„Åó„Å¶„ÅÑ„Åü„Å†„Åë„Åæ„Åô„ÅãÔºü").strip()
FALLBACK_RATE      = os.getenv("FALLBACK_RATE", "„Åü„Å†„ÅÑ„ÅæÂ∞ë„ÅóÊ∑∑„ÅøÂêà„Å£„Å¶„ÅÑ„Çã„Åü„ÇÅ„ÄÅ„Åô„Åê„Å´ÂøúÁ≠î„Åß„Åç„Åæ„Åõ„Çì„Åß„Åó„Åü„ÄÇÊôÇÈñì„Çí„Åä„ÅÑ„Å¶„Åã„Çâ„ÄÅ„ÇÇ„ÅÜ‰∏ÄÂ∫¶„ÅäË©¶„Åó„Åè„Å†„Åï„ÅÑ„ÄÇ").strip()
FALLBACK_SENSITIVE = os.getenv("FALLBACK_SENSITIVE", "").strip()


def estimate_tokens(messages: List[Dict]) -> int:
        """
        „Åñ„Å£„Åè„ÇäÊé®ÂÆö„ÄÇ„É¢„Éá„É´„Å´ÂØæÂøú„Åô„Çã„Ç®„É≥„Ç≥„Éº„ÉÄ„ÅåÁÑ°„Åë„Çå„Å∞ cl100k „Åß‰ª£Êõø„ÄÇ
        """
        try:
            enc = tiktoken.encoding_for_model(MODEL)
        except Exception:
            enc = tiktoken.get_encoding("cl100k_base")

        total = 0
        for m in messages:
            content = m.get("content") or ""
            if isinstance(content, list):
                text = " ".join([p.get("text","") for p in content if isinstance(p, dict)])
            else:
                text = str(content)
            total += len(enc.encode(m.get("role","") + ": " + text))
        return total

def _system_block(title: str, body: str) -> Dict:
        return {"role": "system", "content": f"### {title}\n{body}".strip()}

def summarize_text_block(text: str, lang: str = "ja") -> str:
        """
        Âè§„ÅÑÂå∫Èñì„ÇíË¶ÅÁ¥ÑÔºà‰∫ãÂÆü/Ê±∫ÂÆö/„Çø„Çπ„ÇØ/Âõ∫ÊúâÂêçË©û„Å´Áµû„Å£„Å¶Áü≠ÊñáÂåñÔºâ
        """
        system = "You are a careful summarizer."
        prompt = (
            "‰ª•‰∏ã„ÅÆ‰ºöË©±„É≠„Ç∞„Çí„ÄÅ‰∫ãÂÆü„ÉªÊ±∫ÂÆö‰∫ãÈ†Ö„ÉªÊú™Ê±∫„Çø„Çπ„ÇØ„ÉªÂõ∫ÊúâÂêçË©û„Å´Áµû„Å£„Å¶"
            "Êó•Êú¨Ë™û„Åß150„Äú250„Éà„Éº„ÇØ„É≥Á®ãÂ∫¶„Å´Ë¶ÅÁ¥Ñ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇÊå®Êã∂„ÇÑÂÜóÈï∑Ë°®Áèæ„ÅØÂâäÈô§„ÄÇÂºïÁî®„ÅØ‰∏çË¶Å„ÄÇ"
        )
        # Chat Completions „Åß„ÇÇ Responses „Åß„ÇÇOK„Å´„Åô„ÇãÁ∞°ÊòìÂÆüË£Ö
        try:
            resp = client.chat.completions.create(
                model=MODEL_SUMMARY,
                messages=[
                    {"role":"system","content":system},
                    {"role":"user","content": prompt + "\n\n" + text}
                ],
                temperature=0.2,
                max_tokens=400,
            )
            return resp.choices[0].message.content.strip()
        except Exception:
            # „Éï„Çß„Ç§„É´„Çª„Éº„Éï„Åß„Åù„ÅÆ„Åæ„ÅæÂàá„ÇäË©∞„ÇÅ
            return text[:1500]

def shrink_context(msgs: List[Dict], target_tokens: int) -> List[Dict]:
        """
        „ÅÑ„Å°„Å∞„ÇìÂè§„ÅÑ user/assistant „Éö„Ç¢„Åã„Çâ„Åæ„Å®„ÇÅ„Å¶Ë¶ÅÁ¥Ñ„Å´ÁΩÆÊèõ„Åó„Å¶„ÅÑ„Åè„ÄÇ
        """
        # system ‰ª•Â§ñ„ÅÆÊúÄÂàù„ÅÆÈÄ£Á∂ö„Éñ„É≠„ÉÉ„ÇØ„Çí„Åæ„Å®„ÇÅ„Çã
        sys_count = sum(1 for m in msgs if m["role"] == "system")
        # Âè§„ÅÑÈ†Ü„ÅßÈùûsystem„ÇíÂèéÈõÜ
        non_sys = [i for i, m in enumerate(msgs) if m["role"] != "system"]
        if not non_sys:
            return msgs

        start = non_sys[0]  # ‰∏ÄÁï™Âè§„ÅÑ
        # „ÅÇ„ÇãÁ®ãÂ∫¶„ÅÆÂ°äÔºà‰æã: 4„Äú6„É°„ÉÉ„Çª„Éº„Ç∏Ôºâ„ÅßË¶ÅÁ¥Ñ
        end = min(start + 6, len(msgs))
        chunk = msgs[start:end]
        text = "\n\n".join([f"{m['role']}: {m['content']}" for m in chunk])
        summary = summarize_text_block(text)

        # ÁΩÆÊèõÔºàsystem„ÅÆÁõ¥Âæå„ÅÇ„Åü„Çä„Å´„ÄåSummarized history„Äç„Éñ„É≠„ÉÉ„ÇØ„ÇíÂ∑Æ„ÅóËæº„ÇÄÔºâ
        new_msgs = msgs[:start] + [_system_block("Summarized history (older turns)", summary)] + msgs[end:]

        # „Åæ„Å†Â§ß„Åç„Åë„Çå„Å∞Áπ∞„ÇäËøî„Åô
        while estimate_tokens(new_msgs) > target_tokens:
            # Ê¨°„ÅÆÂè§„ÅÑÂ°ä
            non_sys2 = [i for i, m in enumerate(new_msgs) if m["role"] != "system"]
            if len(non_sys2) <= 6:
                break
            start2 = non_sys2[0]
            end2 = min(start2 + 6, len(new_msgs))
            chunk2 = new_msgs[start2:end2]
            text2 = "\n\n".join([f"{m['role']}: {m['content']}" for m in chunk2])
            summary2 = summarize_text_block(text2)
            new_msgs = new_msgs[:start2] + [_system_block("Summarized history (older turns)", summary2)] + new_msgs[end2:]

            if all(m["role"] == "system" for m in new_msgs):
                break

        return new_msgs

def force_shrink(msgs: List[Dict]) -> List[Dict]:
        """
        ÊúÄÁµÇÊâãÊÆµÔºöÊó¢Â≠ò„ÅÆ„Çµ„Éû„É™„ÇíÂÜçË¶ÅÁ¥Ñ or Èùûsystem„Çí„Åª„ÅºËêΩ„Å®„Åô
        """
        # Êó¢Â≠ò summary „ÇíÁü≠Á∏Æ
        for i, m in enumerate(msgs):
            if m["role"] == "system" and "summary" in m.get("content","").lower():
                m["content"] = m["content"][:2000]  # Â§ßÈõëÊää„Å´ÂàáË©∞„ÇÅ
        # „Åæ„Å†„ÉÄ„É°„Å™„ÇâÁõ¥Ëøë‰ª•Â§ñ„ÅÆ user/assistant „ÇíÂâä„Çã
        if estimate_tokens(msgs) > TARGET_BUDGET:
            keep = []
            # system ÂÖ®ÈÉ® + Áõ¥Ëøë2 user/assistant
            non_sys = [m for m in msgs if m["role"] != "system"]
            keep_non_sys = non_sys[-4:]
            keep = [m for m in msgs if m["role"] == "system"] + keep_non_sys
            return keep
        return msgs

def build_messages(
        system_prompt: str,
        profile_bullets: List[str],
        running_summary: str,
        recent_turns: List[Tuple[str, str]],
        hard_cap_tokens: int = TARGET_BUDGET,
        ) -> List[Dict]:
        """
        recent_turns: [(role, content)] Âè§‚ÜíÊñ∞„ÅÆÈ†Ü „Åß„ÇÇ Êñ∞‚ÜíÂè§„Åß„ÇÇOK„ÄÇÂæå„Åß‰∏¶„Å≥Êõø„Åà„Çã„ÄÇ
        """
        msgs: List[Dict] = []
        # 1) ÊúÄÂ∞è„Çª„ÉÉ„Éà „Åæ„Åö CORE / ROLE / SYSTEM „Çí„Åì„ÅÆÈ†Ü„ÅßÂ∑Æ„ÅóËæº„ÇÄÔºàÂ≠òÂú®„Åô„Çã„ÇÇ„ÅÆ„ÅÆ„ÅøÔºâ
        if CORE_PROMPT:
            msgs.append({"role": "system", "content": CORE_PROMPT[:4000]})
        if ROLE_PROMPT:
            msgs.append({"role": "system", "content": ROLE_PROMPT[:4000]})
        if system_prompt:
            msgs.append({"role": "system", "content": system_prompt.strip()[:4000]})


        if profile_bullets:
            profile_text = "- " + "\n- ".join(profile_bullets[:10])
            msgs.append(_system_block("User profile (concise)", profile_text))

        if running_summary:
            msgs.append(_system_block("Conversation background (summary)", running_summary))

        # 2) Áõ¥ËøëK„Çø„Éº„É≥ÔºàÂ§ö„ÇÅ„Å´ÂÖ•„Çå„Å¶„ÄÅ‰∫àÁÆó„ÅßÂâä„ÇãÔºâ
        #    Êñ∞„Åó„ÅÑÈ†Ü„ÅßÊúÄÂæå„Å´Êù•„Çã„Çà„ÅÜ„Å´Êï¥Âàó
        turns = recent_turns[:]  # shallow copy
        # ÔºàÂè§‚ÜíÊñ∞Ôºâ„ÅÆÊÉ≥ÂÆö„Å´„Åó„Å¶„ÄÅÊñ∞„Åó„ÅÑ„ÇÇ„ÅÆ„ÅåÂæå„Å´‰∏¶„Å∂„Çà„ÅÜ„Å´„Åó„Å¶„Åä„Åè
        for role, content in turns:
            msgs.append({"role": role, "content": content})

        # 3) Ë∂ÖÈÅé„ÉÅ„Çß„ÉÉ„ÇØ ‚Üí Á∏ÆÁ¥Ñ
        tokens = estimate_tokens(msgs)
        if tokens > hard_cap_tokens:
            msgs = shrink_context(msgs, hard_cap_tokens)

        # ÂøµÊäº„Åó
        while estimate_tokens(msgs) > hard_cap_tokens and any(m["role"] != "system" for m in msgs):
            msgs = force_shrink(msgs)

        return msgs

def _extract_usage_from_chat_completion(resp):
    try:
        u = getattr(resp, "usage", None)
        if not u:
            return (None, None, None)

        def uget(key):
            try:
                v = getattr(u, key)
                if v is not None:
                    return v
            except Exception:
                pass
            if isinstance(u, dict):
                return u.get(key)
            return None

        inp = uget("prompt_tokens")
        out = uget("completion_tokens")
        tot = uget("total_tokens")

        if inp is None:
            inp = uget("input_tokens")
        if out is None:
            out = uget("output_tokens")
        if tot is None and (inp is not None or out is not None):
            tot = (inp or 0) + (out or 0)

        return (
            int(inp) if inp is not None else None,
            int(out) if out is not None else None,
            int(tot) if tot is not None else None,
        )
    except Exception:
        return (None, None, None)


def generate_chat(messages: List[Dict], max_tokens: int = 1024, temperature: float = 0.2) -> str:
        """
        ÂÆüÈöõ„ÅÆÁîüÊàêÂëº„Å≥Âá∫„Åó„ÄÇcontextË∂ÖÈÅéÊôÇ„ÅØ„Éï„Çß„Ç§„É´„Çª„Éº„Éï„ÅßÁ∏ÆÁ¥ÑÂÜçÈÄÅ„ÄÇ
        """
        try:
            resp = client.chat.completions.create(
                model=MODEL,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
            )

            inp_toks, out_toks, tot_toks = _extract_usage_from_chat_completion(resp)
            if tot_toks is not None:
                _token_meter.add(tot_toks)
            logger.info("üßÆ usage: input=%s output=%s total=%s | last60s=%s",
                    inp_toks, out_toks, tot_toks, _token_meter.last_60s())
            return resp.choices[0].message.content
        except BadRequestError as e:
            # context_length_exceeded ÂØæÂøúÔºöÊúÄÂ∞èÊßãÊàê„Å´Á∏ÆÁ¥Ñ„Åó„Å¶ÂÜçÈÄÅ
            if "context length" in str(e).lower() or "context_length_exceeded" in str(e).lower():
                minimal = force_shrink(messages)
                resp = client.chat.completions.create(
                    model=MODEL,
                    messages=minimal,
                    temperature=temperature,
                    max_tokens=max_tokens,
                )
                inp_toks, out_toks, tot_toks = _extract_usage_from_chat_completion(resp)
                if tot_toks is not None:
                    _token_meter.add(tot_toks)
                logger.info("üßÆ usage: input=%s output=%s total=%s | last60s=%s",
                        inp_toks, out_toks, tot_toks, _token_meter.last_60s())
                return resp.choices[0].message.content
            raise

class TokenMeter:
    def __init__(self):
        self.buf = deque()
        self.total = 0

    def _prune(self, now: float):
        # 60Áßí„Çà„ÇäÂè§„ÅÑË¶ÅÁ¥†„ÇíËêΩ„Å®„Åó„Å¶ÂêàË®à„ÇíË™øÊï¥
        while self.buf and now - self.buf[0][0] > 60.0:
            t, n = self.buf.popleft()
            self.total -= n
            if self.total < 0:
                self.total = 0

    def add(self, tokens: int):
        now = time.time()
        self._prune(now)
        self.buf.append((now, int(tokens)))
        self.total += int(tokens)

    def last_60s(self) -> int:
        self._prune(time.time())
        return self.total

_token_meter = TokenMeter()

class OpenAIClient:
    def __init__(self):
        self.client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        self.model = os.getenv("OPENAI_MODEL", "gpt-4.1-nano")

    def _is_rate_limit(self, err: Exception) -> bool:
        try:
            if isinstance(err, RateLimitError):
                return True
        except Exception:
            pass

        try:
            if isinstance(err, APIStatusError) and getattr(err, "status_code", None) == 429:
                return True
        except Exception:
            pass

        msg = (str(err) or "").lower()
        if "429" in msg or "rate limit" in msg or "too many requests" in msg:
            return True
        if hasattr(err, "status_code") and getattr(err, "status_code") == 429:
            return True
        return False

    def _compose_messages(self, user_message: str):
        messages = []
        if CORE_PROMPT:
            messages.append({"role": "system", "content": CORE_PROMPT})
        if ROLE_PROMPT:
            messages.append({"role": "system", "content": ROLE_PROMPT})
        messages.append({"role": "user", "content": user_message})
        return messages

    def _extract_usage(self, response):
        try:
            u = getattr(response, "usage", None) or {}
            inp = getattr(u, "input_tokens", None) or u.get("input_tokens")
            out = getattr(u, "output_tokens", None) or u.get("output_tokens")
            tot = getattr(u, "total_tokens", None) or u.get("total_tokens")

            # total „ÅåÁÑ°„Åë„Çå„Å∞Âä†ÁÆó„ÅßÊé®ÂÆö
            if tot is None and (inp is not None or out is not None):
                tot = (inp or 0) + (out or 0)

            # ‰Ωï„ÇÇÂèñ„Çå„Å™„ÅÑÂ†¥Âêà„ÅØ None Ëøî„Åó
            if inp is None and out is None and tot is None:
                return (None, None, None)

            return (int(inp) if inp is not None else None,
                    int(out) if out is not None else None,
                    int(tot) if tot is not None else None)
        except Exception:
            return (None, None, None)

    def _friendly_fallback(self, err: Exception | None, kind: str = "generic") -> str:
        if err and self._is_rate_limit(err):
            return FALLBACK_RATE
        if kind == "sensitive":
            return FALLBACK_SENSITIVE
        if kind == "rate":
            return FALLBACK_RATE
        msg = (str(err) or "").lower() if err else ""
        if ("policy" in msg or ("content" in msg and "filter" in msg)):
            return FALLBACK_SENSITIVE
        if ("rate" in msg or "429" in msg or "timeout" in msg or "temporarily" in msg or "503" in msg):
            return FALLBACK_RATE
        return FALLBACK_GENERIC

    def get_reply(self, user_message: str, previous_response_id: str=None):
        try:
            messages = self._compose_messages(user_message)

            if previous_response_id:
                response = self.client.responses.create(
                    model=self.model,
                    input=messages,
                    previous_response_id=previous_response_id
                )
            else:
                response = self.client.responses.create(
                    model=self.model,
                    input=messages,
                    store=True
                )

            ai_message  = (getattr(response, "output_text", "") or "").strip()
            response_id = getattr(response, "id", None)

            inp_toks, out_toks, tot_toks = self._extract_usage(response)
            if tot_toks is not None:
                _token_meter.add(tot_toks)

            logger.info(
                "üßÆ usage: input=%s output=%s total=%s | last60s=%s",
                inp_toks, out_toks, tot_toks, _token_meter.last_60s()
            )

            if not ai_message:
                logger.warning("‚ö†Ô∏è output_text is empty; using generic fallback")
                return self._friendly_fallback(None, "generic"), (response_id or previous_response_id)

            return ai_message, response_id

        except Exception as e:
            logger.exception("üî• OpenAIClient#get_reply failed")
            if self._is_rate_limit(e):
                return self._friendly_fallback(e, "rate"), previous_response_id
            return self._friendly_fallback(e), previous_response_id
